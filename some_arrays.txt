@jax.jit
def feed_forward(inputs: jnp.ndarray, neurons: jnp.ndarray) -> jnp.ndarray:
    """
    Calculates the continous output of the network

    Parameters
    inputs - the input data
    neurons - the network
    
    Returns
    the continuous output
    """
    xs = jnp.ones((i_3,i_4))
    xs = xs.at[0].set(jnp.pad(inputs,(0, i_4-len(inputs)), mode="constant", constant_values=1))
    for layer_i in range(i_1-1):
        xs = xs.at[layer_i+1, :arch[layer_i+1]].set(jax.vmap(forward, in_axes=(0, None))(neurons[layer_i], xs)[:arch[layer_i+1]])
    return jax.vmap(forward, in_axes=(0, None))(neurons[i_1-1], xs)[:outs]

@jax.jit
def feed_forward_disc(inputs: jnp.ndarray, neurons: jnp.ndarray) -> jnp.ndarray:
    """
    Calculates the discrete output of the network

    Parameters
    inputs - the input data
    neurons - the network
    
    Returns
    the discrete output
    """
    xs = jnp.ones((i_3,i_4))
    xs = xs.at[0].set(jnp.pad(inputs,(0, i_4-len(inputs)), mode="constant", constant_values=1))
    for layer_i in range(i_1-1):
        xs = xs.at[layer_i+1, :arch[layer_i+1]].set(jax.vmap(forward_disc, in_axes=(0, None))(neurons[layer_i], xs)[:arch[layer_i+1]])
    return jax.vmap(forward_disc, in_axes=(0, None))(neurons[i_1-1], xs)[:outs]

def feed_forward_disc_print(inputs: jnp.ndarray, neurons: jnp.ndarray) -> jnp.ndarray:
    """
    Calculates the discrete output of the network, whilst outputting useful debugging data

    Parameters
    inputs - the input data
    neurons - the network
    
    Returns
    the discrete output
    """
    xs = jnp.ones((i_3,i_4))
    xs = xs.at[0].set(jnp.pad(inputs,(0, i_4-len(inputs)), mode="constant", constant_values=1))
    jax.debug.print("inputs:{}", inputs)
    jax.debug.print("{}", xs)
    for layer_i in range(i_1-1):
        xs = xs.at[layer_i+1, :arch[layer_i+1]].set(jax.vmap(forward_disc, in_axes=(0, None))(neurons[layer_i], xs)[:arch[layer_i+1]])
        jax.debug.print("{}", xs)
    jax.debug.print("{}", jax.vmap(forward_disc, in_axes=(0, None))(neurons[i_1-1], xs)[:outs])
    return jax.vmap(forward_disc, in_axes=(0, None))(neurons[i_1-1], xs)[:outs]

def get_weights(layer: int, arch: List[int], sigma: jnp.ndarray, k: jnp.ndarray) -> jnp.ndarray:
    """
    Returns the weights of a given neuron

    Parameters
    layer - the layer that the neuron is in
    arch - a list representing the architecture
    sigma - the standard deviation of the normal distribution
    k - a rescaling factor (it's dependent on sigma, but the mathematical relation isn't clear, so I pass it in separately)
    
    Returns
    a 2d jnp array of the weights, which represents the wires going into a certain neuron
    """
    global key
    weights = jnp.ones((i_3,i_4)) * -jnp.inf
    # layer lists, each with arch[i] elements
    # so this is a 2D list of floats
    # or a 1D list of jnp arrays
    if global_weights == 'g':
        n = global_n
    else:
        n = sum(arch[:layer])
    mu = -jnp.log(n-1)/k
    for i in range(layer):
        inner_layer = sigma * jax.random.normal(jax.random.key(key), (arch[i])) + mu #type: ignore
        weights = weights.at[i].set(jnp.pad(inner_layer, (0, i_4-arch[i]), mode="constant", constant_values=-jnp.inf))
        key = random.randint(0, 10000)
    return weights

def initialise(arch: List[int], sigma: jnp.ndarray, k: jnp.ndarray) -> List[jnp.ndarray]:
    """
    initialises the network

    Parameters
    arch - a list representing the architecture
    sigma - the standard deviation of the normal distribution
    k - a rescaling factor (it's dependent on sigma, but the mathematical relation isn't clear, so I pass it in separately)
    
    Returns
    the network
    """
    neurons = []
    for i1 in range(1, len(arch)):
        layer = jnp.ones((arch[i1], i_3, i_4))
        for i2 in range(arch[i1]):
            layer = layer.at[i2].set(get_weights(i1, arch, sigma, k))
        neurons.append(layer)
    return neurons
